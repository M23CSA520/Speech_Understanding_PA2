# -*- coding: utf-8 -*-
"""M23CSA520_Q1_SUPA2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LxXYdYMsLiz5zqu3GnT81rSxB9qEYLEa
"""

#Question 1: Speech Enhancement
#Task I Download ( Vox celeb 1 Dataset)
!pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib

from google.colab import drive
# Mount Google Drive
drive.mount('/content/drive')


#Downloading Files from Shared Google Drive
from google.colab import auth
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import os
import io
import zipfile

# Authenticate with Google Drive
auth.authenticate_user()
drive_service = build('drive', 'v3')

# Function to download a folder from Google Drive
def download_gdrive_folder(folder_id, destination):
    query = f"'{folder_id}' in parents and mimeType != 'application/vnd.google-apps.folder'"
    results = drive_service.files().list(q=query).execute()
    files = results.get('files', [])

    if not os.path.exists(destination):
        os.makedirs(destination)

    for file in files:
        file_id = file['id']
        file_name = file['name']
        request = drive_service.files().get_media(fileId=file_id)
        file_path = os.path.join(destination, file_name)

        with open(file_path, 'wb') as f:
            downloader = MediaIoBaseDownload(f, request)
            done = False
            while not done:
                status, done = downloader.next_chunk()
                print(f"Downloading {file_name}: {int(status.progress() * 100)}% complete.")

    print(f"Downloaded {len(files)} files to {destination}")

# Replace with the actual shared folder ID (from the link)
folder_id = "1iZJy4mSV2GZYyVXNWW43BZ6GUOo3DcO0"  # Extracted from the shared link
destination_path = "/content/VoxCeleb_Dataset"

download_gdrive_folder(folder_id, destination_path)

import zipfile
vox1_path = os.path.join(destination_path, "vox1")
print("Vox 1 Path : ", vox1_path )
os.makedirs(vox1_path, exist_ok=True)
# Function to extract zip files
def extract_zip(zip_file, extract_to):
    with zipfile.ZipFile(zip_file, 'r') as zip_ref:
        zip_ref.extractall(extract_to)
        print(f"Extracted {zip_file} to {extract_to}")

# Extract the datasets
extract_zip(os.path.join(destination_path, "vox1_test_wav.zip"), vox1_path)

!pip install datasets
!pip install transformers

from transformers import Wav2Vec2FeatureExtractor,WavLMModel
import torch

# Load pre-trained model and processor
model_name = "microsoft/wavlm-base"
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)
model = WavLMModel.from_pretrained(model_name)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

import torchaudio
import numpy as np

from tqdm import tqdm

def compute_embeddings_batch(audio_files, model, feature_extractor, device, batch_size=4):
    """
    Computes embeddings for a list of audio files in batches.

    Args:
        audio_files (list): List of audio file paths.
        model: Pre-trained model.
        feature_extractor: Feature extractor for preprocessing audio.
        device: Device (CPU or GPU).
        batch_size (int): Number of files to process in each batch.

    Returns:
        embeddings (dict): Dictionary mapping file paths to their embeddings.
    """
    embeddings = {}
    for i in tqdm(range(0, len(audio_files), batch_size), desc="Processing batches"):
        batch_files = audio_files[i:i + batch_size]
        batch_waveforms = []

        # Load waveforms for the batch
        for file in batch_files:
            waveform, _ = torchaudio.load(file)
            batch_waveforms.append(waveform.squeeze().numpy())

        # Preprocess batch
        inputs = feature_extractor(batch_waveforms, return_tensors="pt", sampling_rate=16000, padding=True)
        inputs = {key: value.to(device) for key, value in inputs.items()}

        # Compute embeddings
        with torch.no_grad():
            outputs = model(**inputs)
        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()

        # Store embeddings
        for file, embedding in zip(batch_files, batch_embeddings):
            embeddings[file] = embedding

    return embeddings

def parse_trial_pairs(trial_pairs_file):
    """
    Parses trial pairs from the given file.

    Args:
        trial_pairs_file (str): Path to the trial pairs file.

    Returns:
        pairs (list): List of trial pairs (audio_path_1, audio_path_2).
        labels (list): List of labels (1 for genuine, 0 for impostor).
    """
    trials = []
    with open(trial_pairs_file, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) == 3:  # Ensure the line has exactly 3 parts
                label, audio_path_1, audio_path_2 = parts
                trials.append((int(label), audio_path_1, audio_path_2))
            else:
                print(f"Skipping invalid line: {line.strip()}")
    return trials

from sklearn.metrics.pairwise import cosine_similarity

# Compute cosine similarity scores for trial pairs
def compute_similarity_scores(trials, data_dir, model, feature_extractor, device, batch_size=8):
    """
    Computes cosine similarity scores for trial pairs by processing audio files in batches.

    Args:
        trials (list): List of trial pairs (audio_path_1, audio_path_2).
        data_dir (str): Directory containing audio files.
        model: Pre-trained model.
        feature_extractor: Feature extractor for preprocessing audio.
        device: Device (CPU or GPU).
        batch_size (int): Number of files to process in each batch.

    Returns:
        scores (list): List of similarity scores.
    """
    # Step 1: Extract unique audio files
    unique_files = set()
    for label,audio_path_1, audio_path_2 in trials:
        unique_files.add(f"{data_dir}/{audio_path_1}")
        unique_files.add(f"{data_dir}/{audio_path_2}")
    unique_files = list(unique_files)

    # Step 2: Compute embeddings for all unique files in batches
    embeddings = compute_embeddings_batch(unique_files, model, feature_extractor, device, batch_size)

    # Step 3: Compute similarity scores for all pairs
    scores = []
    labels = []
    for label, audio_path_1, audio_path_2 in tqdm(trials, desc="Computing similarity scores"):
        full_path_1 = f"{data_dir}/{audio_path_1}"
        full_path_2 = f"{data_dir}/{audio_path_2}"

        if full_path_1 not in embeddings or full_path_2 not in embeddings:
            print(f"Missing embedding for file: {full_path_1} or {full_path_2}")
            scores.append(None)  # Append None for missing embeddings
            continue

        embedding_1 = embeddings[full_path_1]
        embedding_2 = embeddings[full_path_2]

        # Compute cosine similarity
        similarity = cosine_similarity([embedding_1], [embedding_2])[0][0]
        scores.append(similarity)
        labels.append(label)

    return scores, labels

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import os
os.makedirs("results", exist_ok=True)

# Evaluate metrics (EER, TAR@1%FAR, ROC AUC)
def evaluate_metrics(scores, labels):
    """
    Evaluates the model using EER, TAR@1%FAR, and plots the ROC curve.

    Args:
        scores (list): Similarity scores between trial pairs.
        labels (list): Ground truth labels (1 for genuine, 0 for impostor).

    Returns:
        roc_auc (float): Area Under the Curve (AUC).
        eer (float): Equal Error Rate (EER).
        tar_at_1_far (float): True Acceptance Rate at 1% False Acceptance Rate.
    """
    # Filter out None values (missing embeddings)
    valid_scores = [score for score in scores if score is not None]
    valid_labels = [label for label, score in zip(labels, scores) if score is not None]

    # Compute ROC curve and AUC
    fpr, tpr, thresholds = roc_curve(valid_labels, valid_scores)
    roc_auc = auc(fpr, tpr)

    # Plot ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.savefig("results/roc_curve.png")
    plt.show()

    # Compute EER
    eer_threshold = thresholds[np.nanargmin(np.abs(fpr - (1 - tpr)))]
    eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]
    print(f"EER: {eer * 100:.2f}%")

    # Compute TAR@1%FAR
    far_1_index = np.argmin(np.abs(fpr - 0.01))
    tar_at_1_far = tpr[far_1_index]
    print(f"TAR@1%FAR: {tar_at_1_far * 100:.2f}%")

    return roc_auc, eer, tar_at_1_far,eer_threshold

import seaborn as sns

# Plot cosine similarity matrix
def plot_cosine_similarity_matrix(embeddings, labels):
    """
    Plots a cosine similarity matrix for a subset of embeddings.

    Args:
        embeddings (list): List of embeddings.
        labels (list): Corresponding labels.
    """
    num_samples = len(embeddings)
    similarity_matrix = np.zeros((num_samples, num_samples))

    for i in range(num_samples):
        for j in range(num_samples):
            similarity_matrix[i, j] = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]

    # Plot heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(similarity_matrix, annot=False, cmap="coolwarm", xticklabels=labels, yticklabels=labels)
    plt.title("Cosine Similarity Matrix")
    plt.savefig("results/cosine_similarity_matrix.png")
    plt.show()

def compute_identification_accuracy(labels, scores, threshold=0.5):
    predictions = [1 if score >= threshold else 0 for score in scores]
    correct = sum(1 for pred, label in zip(predictions, labels) if pred == label)
    accuracy = correct / len(labels) * 100  # Convert to percentage
    return accuracy

#Q1. I Vox Celeb 1 on pretrained
def main():
    # Paths
    trial_pairs_file = "/content/VoxCeleb_Dataset/vox1/trial_pair_vox_celeb1_cleaned.txt"
    data_dir = "/content/VoxCeleb_Dataset/vox1/wav"

    # Parse trial pairs
    trials = parse_trial_pairs(trial_pairs_file)

    print(f"Number of pairs: {len(trials)}")

    # Compute similarity scores using batch processing
    scores,labels = compute_similarity_scores(trials, data_dir, model, feature_extractor, device, batch_size=4)

    # Evaluate metrics
    roc_auc, eer, tar_at_1_far,eer_threshold = evaluate_metrics(scores, labels)

    id_accuracy = compute_identification_accuracy(labels, scores, threshold=eer_threshold)
    print(f"Speaker Identification Accuracy: {id_accuracy:.2f}%")

    # Plot cosine similarity matrix for a subset of embeddings
    unique_files = set()
    labels1 = []
    for labels,audio_path_1, audio_path_2 in trials[:10]:  # Use the first 10 pairs for visualization
        unique_files.add(f"{data_dir}/{audio_path_1}")
        unique_files.add(f"{data_dir}/{audio_path_2}")
        labels1.append(labels)
    unique_files = list(unique_files)

    embeddings = compute_embeddings_batch(unique_files, model, feature_extractor, device, batch_size=4)
    plot_cosine_similarity_matrix(list(embeddings.values()), labels1[:10])

if __name__ == "__main__":
    main()

import torch
torch.cuda.empty_cache()

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

folder_id1 = "1Z88STD-m3Ch2DWIdjTR3mSku47nQBix0"  # Extracted from the shared link
destination_path = "/content/VoxCeleb_Dataset"

download_gdrive_folder(folder_id1, destination_path)

import zipfile
import os
vox2_path = os.path.join(destination_path, "vox2")
print("Vox 2 Path : ", vox2_path )
os.makedirs(vox2_path, exist_ok=True)
# Function to extract zip files
def extract_zip(zip_file, extract_to):
    with zipfile.ZipFile(zip_file, 'r') as zip_ref:
        zip_ref.extractall(extract_to)
        print(f"Extracted {zip_file} to {extract_to}")

# Extract the datasets
extract_zip(os.path.join(destination_path, "vox2_test_aac.zip"), vox2_path)

extract_zip(os.path.join(destination_path, "vox2_test_txt.zip"), vox2_path)

import os
import numpy as np
from tqdm import tqdm
import torch
import torch.nn as nn
import torchaudio
from torch.utils.data import Dataset, DataLoader
from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor
from sklearn.metrics import roc_curve, auc
from sklearn.metrics.pairwise import cosine_similarity
from peft import get_peft_model, LoraConfig

import os
data_dir = "/content/VoxCeleb_Dataset/vox2/aac"
# Prepare VoxCeleb2 dataset
def prepare_voxceleb2_dataset(data_dir, num_train_speakers=100):
    """
    Prepares VoxCeleb2 dataset by splitting into training and testing sets.

    Args:
        data_dir (str): Path to the VoxCeleb2 dataset.
        num_train_speakers (int): Number of speakers to use for training.

    Returns:
        train_files (list): List of training file paths.
        train_labels (list): Corresponding speaker IDs for training files.
        test_files (list): List of testing file paths.
        test_labels (list): Corresponding speaker IDs for testing files.
    """
    all_speakers = sorted(os.listdir(data_dir))  # Sort speakers alphabetically
    train_speakers = all_speakers[:num_train_speakers]
    test_speakers = all_speakers[num_train_speakers:]

    train_files, train_labels = [], []
    test_files, test_labels = [], []

    for speaker in train_speakers:
        speaker_dir = os.path.join(data_dir, speaker)
        for root, _, files in os.walk(speaker_dir):
            for file in files:
                if file.endswith(".m4a"):
                    train_files.append(os.path.join(root, file))
                    train_labels.append(speaker)

    for speaker in test_speakers:
        speaker_dir = os.path.join(data_dir, speaker)
        for root, _, files in os.walk(speaker_dir):
            for file in files:
                if file.endswith(".m4a"):
                    test_files.append(os.path.join(root, file))
                    test_labels.append(speaker)

    return train_files, train_labels, test_files, test_labels

import torch
torch.cuda.empty_cache()

# Function to load .m4a files with resampling
def load_audio(file_path, target_sample_rate=16000):
    """
    Loads an audio file and resamples it to the target sample rate.

    Args:
        file_path (str): Path to the audio file.
        target_sample_rate (int): Desired sample rate (default: 16000 Hz).

    Returns:
        waveform (torch.Tensor): Audio waveform.
        sample_rate (int): Sample rate of the loaded audio.
    """
    waveform, sample_rate = torchaudio.load(file_path)
    if sample_rate != target_sample_rate:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)
        waveform = resampler(waveform)
    return waveform.squeeze(), target_sample_rate

# Dataset class for handling .m4a files
class SpeakerDataset(Dataset):
    def __init__(self, file_paths, labels, feature_extractor, speaker_to_label, max_length=160000):  # Max length = 10 seconds
        self.file_paths = file_paths
        self.labels = labels
        self.feature_extractor = feature_extractor
        self.speaker_to_label = speaker_to_label
        self.max_length = max_length

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        file_path = self.file_paths[idx]
        label = self.labels[idx]
        label = self.speaker_to_label[label]  # Map speaker ID to integer

        # Load and resample audio
        waveform, _ = load_audio(file_path, target_sample_rate=16000)

        # Truncate or pad to max_length
        if waveform.size(0) > self.max_length:
            waveform = waveform[:self.max_length]  # Truncate
        else:
            padding = self.max_length - waveform.size(0)
            waveform = torch.nn.functional.pad(waveform, (0, padding))  # Pad with zeros

        # Preprocess with feature extractor
        inputs = self.feature_extractor(waveform.numpy(), return_tensors="pt", sampling_rate=16000)
        return inputs["input_values"].squeeze(0), label

# ArcFace Loss Implementation
class ArcFaceLoss(nn.Module):
    def __init__(self, num_classes, embedding_size, margin=0.5, scale=64):
        super(ArcFaceLoss, self).__init__()
        self.num_classes = num_classes
        self.embedding_size = embedding_size
        self.margin = margin
        self.scale = scale
        self.weight = nn.Parameter(torch.randn(num_classes, embedding_size))

    def forward(self, embeddings, labels):
        # Normalize embeddings and weights
        embeddings = torch.nn.functional.normalize(embeddings, dim=1)
        weight = torch.nn.functional.normalize(self.weight, dim=1)

        # Compute cosine similarity
        cos_theta = torch.matmul(embeddings, weight.t())

        # Add margin to angles corresponding to the true class
        one_hot = torch.zeros_like(cos_theta)
        one_hot.scatter_(1, labels.view(-1, 1).long(), 1)
        cos_theta_m = cos_theta - self.margin * one_hot

        # Scale logits and compute cross-entropy loss
        logits = self.scale * cos_theta_m
        loss = torch.nn.functional.cross_entropy(logits, labels)
        return loss

# Model definition with classification head
class WavLMForSpeakerVerification(nn.Module):
    def __init__(self, model_name, num_speakers):
        super(WavLMForSpeakerVerification, self).__init__()
        self.wavlm = WavLMModel.from_pretrained(model_name)
        self.classifier = nn.Linear(self.wavlm.config.hidden_size, num_speakers)

    def forward(self, input_values, attention_mask=None):
        outputs = self.wavlm(input_values, attention_mask=attention_mask)
        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling
        logits = self.classifier(embeddings)
        return embeddings, logits

    def get_hidden_size(self):
        """Helper method to get the hidden size of the underlying WavLM model."""
        return self.wavlm.config.hidden_size

# Training loop
def train_model(model, train_loader, test_loader, arcface_loss, optimizer, num_epochs=5):
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        for batch_inputs, batch_labels in tqdm(train_loader):
            print(f"Batch Inputs Shape: {batch_inputs.shape}, Batch Labels: {batch_labels}")
            batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)

            # Forward pass
            embeddings, logits = model(batch_inputs)
            assert not torch.isnan(embeddings).any(), "Embeddings contain NaN values!"
            assert not torch.isinf(embeddings).any(), "Embeddings contain Inf values!"

            # Compute loss
            loss = arcface_loss(embeddings, batch_labels)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}")

        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_inputs, batch_labels in test_loader:
                batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)
                embeddings, logits = model(batch_inputs)
                loss = arcface_loss(embeddings, batch_labels)
                val_loss += loss.item()

        print(f"Validation Loss: {val_loss / len(test_loader)}")

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

voxceleb2_dir = "/content/VoxCeleb_Dataset/vox2/aac"
trial_pairs_file = "/content/VoxCeleb_Dataset/vox1/trial_pair_vox_celeb1_cleaned.txt"
data_dir = "/content/VoxCeleb_Dataset/vox1/wav"

#Q1. I Fine tuning and training on Vox Celeb2
import random

# Main function
def main():
    # Paths
    voxceleb2_dir = "/content/VoxCeleb_Dataset/vox2/aac"
    trial_pairs_file = "/content/VoxCeleb_Dataset/vox1/trial_pair_vox_celeb1_cleaned.txt"
    data_dir = "/content/VoxCeleb_Dataset/vox1/wav"

    # Prepare VoxCeleb2 dataset
    train_files, train_labels, test_files, test_labels = prepare_voxceleb2_dataset(voxceleb2_dir)
    # Create label encoder
    unique_speakers = sorted(set(train_labels + test_labels))  # Combine train and test labels
    speaker_to_label = {speaker: idx for idx, speaker in enumerate(unique_speakers)}

    # Create datasets
    train_dataset = SpeakerDataset(train_files[:5000], train_labels[:5000], feature_extractor,speaker_to_label)
    test_dataset = SpeakerDataset(test_files, test_labels, feature_extractor,speaker_to_label)

    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)
    if torch.cuda.is_available():
       device = torch.device("cuda")
    else:
        device = torch.device("cpu")
    # Initialize model
    model_name = "microsoft/wavlm-base"
    num_speakers = len(set(train_labels))  # Number of unique speakers in the training set
    model = WavLMForSpeakerVerification(model_name=model_name, num_speakers=num_speakers).to(device)
    print("Model hidden size:", model.get_hidden_size())
    # Add LoRA
    lora_config = LoraConfig(
        r=8,  # Rank of the low-rank matrix
        lora_alpha=16,
        target_modules=["attention.q_proj", "attention.k_proj", "attention.v_proj", "attention.out_proj"],  # Apply LoRA only to the classifier
        lora_dropout=0.1
    )
    model = get_peft_model(model, lora_config)

    # Define ArcFace loss
    arcface_loss = ArcFaceLoss(num_classes=num_speakers, embedding_size=model.get_hidden_size()).to(device)

    # Optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    # Train the model
    train_model(model, train_loader, test_loader, arcface_loss, optimizer, num_epochs=5)

    # Save the fine-tuned model
    torch.save(model.state_dict(), "fine_tuned_wavlm.pth")

if __name__ == "__main__":
   main()

torch.cuda.empty_cache()

# Reload the model
# Q1. I Fine tuned model on Vox Celeb 1
model = WavLMForSpeakerVerification(model_name="microsoft/wavlm-base", num_speakers=num_speakers).to(device)
model.load_state_dict(torch.load("fine_tuned_wavlm.pth"))
model.to(device)
model.eval()

# Parse trial pairs
trial_pairs_file = "/content/VoxCeleb_Dataset/vox1/trial_pair_vox_celeb1_cleaned.txt"

trials = parse_trial_pairs(trial_pairs_file)

# Extract unique audio files
unique_files = set()
for label, audio_path_1, audio_path_2 in trials:
    unique_files.add(f"{data_dir}/{audio_path_1}")
    unique_files.add(f"{data_dir}/{audio_path_2}")
unique_files = list(unique_files)

# Compute embeddings
embeddings = compute_embeddings_batch(unique_files, model, feature_extractor, device, batch_size=8)

# Compute similarity scores
scores, labels = compute_similarity_scores(trials, data_dir, embeddings)

# Evaluate metrics
roc_auc, eer, tar_at_1_far,eer_threshold = evaluate_metrics(scores, labels)

id_accuracy = compute_identification_accuracy(labels, scores, threshold=eer_threshold)
print(f"Speaker Identification Accuracy: {id_accuracy:.2f}%")

# Plot cosine similarity matrix for a subset of embeddings
unique_files = set()
labels1 = []
for labels,audio_path_1, audio_path_2 in trials[:10]:  # Use the first 10 pairs for visualization
    unique_files.add(f"{data_dir}/{audio_path_1}")
    unique_files.add(f"{data_dir}/{audio_path_2}")
    labels1.append(labels)
unique_files = list(unique_files)

embeddings = compute_embeddings_batch(unique_files, model, feature_extractor, device, batch_size=8)
plot_cosine_similarity_matrix(list(embeddings.values()), labels1[:10])
# Clear GPU memory
torch.cuda.empty_cache()

import os
import random
import numpy as np
import torchaudio
import torch
from tqdm import tqdm

# Paths
voxceleb2_dir = "/content/VoxCeleb_Dataset/vox2/aac"
output_train_dir = "/content/VoxCeleb_Dataset/multi_speaker_train"
output_test_dir = "/content/VoxCeleb_Dataset/multi_speaker_test"

# Ensure output directories exist
os.makedirs(output_train_dir, exist_ok=True)
os.makedirs(output_test_dir, exist_ok=True)

# Prepare VoxCeleb2 dataset
def prepare_voxceleb2_dataset(data_dir, num_speakers):
    """
    Prepares VoxCeleb2 dataset by splitting into training and testing sets.

    Args:
        data_dir (str): Path to the VoxCeleb2 dataset.
        num_speakers (int): Number of speakers to use for each set.

    Returns:
        train_files (list): List of training file paths.
        test_files (list): List of testing file paths.
    """
    all_speakers = sorted(os.listdir(data_dir))  # Sort speakers alphabetically
    train_speakers = all_speakers[:num_speakers]
    test_speakers = all_speakers[num_speakers:num_speakers * 2]

    train_files, test_files = [], []

    for speaker in train_speakers:
        speaker_dir = os.path.join(data_dir, speaker)
        for root, _, files in os.walk(speaker_dir):
            for file in files:
                if file.endswith(".m4a"):
                    train_files.append(os.path.join(root, file))

    for speaker in test_speakers:
        speaker_dir = os.path.join(data_dir, speaker)
        for root, _, files in os.walk(speaker_dir):
            for file in files:
                if file.endswith(".m4a"):
                    test_files.append(os.path.join(root, file))

    return train_files, test_files

# Mix two utterances
def mix_utterances(file1, file2, output_dir, snr=0):
    """
    Mixes two audio files with a given SNR.

    Args:
        file1 (str): Path to the first audio file.
        file2 (str): Path to the second audio file.
        output_dir (str): Directory to save the mixed audio.
        snr (float): Signal-to-Noise Ratio (default: 0 dB).
    """
    waveform1, sr1 = torchaudio.load(file1)
    waveform2, sr2 = torchaudio.load(file2)

    # Resample if necessary
    if sr1 != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=sr1, new_freq=16000)
        waveform1 = resampler(waveform1)
    if sr2 != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=sr2, new_freq=16000)
        waveform2 = resampler(waveform2)

    # Truncate to the shorter length
    min_length = min(waveform1.size(1), waveform2.size(1))
    waveform1 = waveform1[:, :min_length]
    waveform2 = waveform2[:, :min_length]

    # Adjust SNR
    energy1 = torch.mean(waveform1**2)
    energy2 = torch.mean(waveform2**2)
    scale = 10 ** (snr / 20)  # Convert SNR to linear scale
    waveform2 = waveform2 * torch.sqrt(energy1 / (energy2 * scale))

    # Mix signals
    mixed_waveform = waveform1 + waveform2
    mixed_waveform = mixed_waveform / torch.max(torch.abs(mixed_waveform))


    # Save the mixed audio
    output_path = os.path.join(output_dir, f"{os.path.basename(file1)}_{os.path.basename(file2)}.wav")
    torchaudio.save(output_path, mixed_waveform, 16000)

# Create multi-speaker datasets
def create_multi_speaker_dataset(files, output_dir, num_mixes=100):
    """
    Creates a multi-speaker dataset by mixing utterances.

    Args:
        files (list): List of audio file paths.
        output_dir (str): Directory to save the mixed audio.
        num_mixes (int): Number of mixed audio files to generate.
    """
    for _ in tqdm(range(num_mixes)):
        file1, file2 = random.sample(files, 2)  # Randomly select two files
        mix_utterances(file1, file2, output_dir)

# Prepare datasets
train_files, test_files = prepare_voxceleb2_dataset(voxceleb2_dir, num_speakers=50)

# Create multi-speaker datasets
create_multi_speaker_dataset(train_files, output_train_dir, num_mixes=100)
create_multi_speaker_dataset(test_files, output_test_dir, num_mixes=50)

!pip install speechbrain

from speechbrain.pretrained import SepformerSeparation
import os
# Load SepFormer model
sepformer_model = SepformerSeparation.from_hparams(source="speechbrain/sepformer-wsj02mix", savedir="pretrained_models/sepformer")

# Function to separate speakers
def separate_speakers(mixed_file):
    """
    Separates speakers using the SepFormer model.

    Args:
        mixed_file (str): Path to the mixed audio file.

    Returns:
        separated_sources (list): List of separated waveforms.
    """
    # Load mixed audio
    mixture, sample_rate = torchaudio.load(mixed_file)
    print("Original mixture shape:", mixture.shape)  # Check initial shape

    if sample_rate != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
        mixture = resampler(mixture)

    print("Mixture shape after resampling:", mixture.shape)
    print("Mixture dim",mixture.ndim)

    # Ensure mixture has the shape [batch_size, channels, sequence_length]
    if mixture.ndim == 1:  # Shape: [sequence_length]
        mixture = mixture.unsqueeze(0).unsqueeze(0)  # Add channel and batch dimensions
    elif mixture.ndim == 2:  # Shape: [channels, sequence_length]
        mixture = mixture.unsqueeze(0)  # Add batch dimension
    elif mixture.ndim == 3:  # Shape: [batch_size, channels, sequence_length]
        pass  # Already correct
    else:
        raise ValueError(f"Unexpected shape for mixture: {mixture.shape}")

    print("Shape of mixture after preprocessing:", mixture.shape)

    # Separate sources
    separated_sources = sepformer_model.separate_batch(mixture)
    # Make sure the separated sources are on the CPU
    separated_sources = separated_sources.squeeze(0).cpu()
    print("Shape of separated_sources:", separated_sources.shape)
    return separated_sources

# ... (rest of the code remains the same)
# Evaluate metrics
def evaluate_metrics(original_signal, enhanced_signal):
    """
    Evaluates SIR, SAR, SDR, and PESQ metrics.

    Args:
        original_signal (torch.Tensor): Original clean signal.
        enhanced_signal (torch.Tensor): Enhanced signal.

    Returns:
        metrics (dict): Dictionary containing SIR, SAR, SDR, and PESQ values.
    """
    from speechbrain.utils.metric_stats import MetricStats
    from pesq import pesq

    # Compute SDR
    sdr_metric = MetricStats(metric="si-sdr")
    sdr_metric.append(
        ids=["test"],
        predictions=[enhanced_signal.numpy()],
        targets=[original_signal.numpy()]
    )
    sdr = sdr_metric.summarize()

    # Compute PESQ
    pesq_score = pesq(16000, original_signal.numpy(), enhanced_signal.numpy(), "wb")

    return {
        "SDR": sdr,
        "PESQ": pesq_score
    }

# Process test set
test_mixed_files = [os.path.join(output_test_dir, f) for f in os.listdir(output_test_dir)]
for mixed_file in tqdm(test_mixed_files):
    separated_sources = separate_speakers(mixed_file)

    # Evaluate metrics for each source
    for i, source in enumerate(separated_sources):
        metrics = evaluate_metrics(original_signal=source, enhanced_signal=source)
        print(f"Source {i+1} Metrics: {metrics}")

# Load fine-tuned model
model_name="microsoft/wavlm-base"
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)
model = WavLMForSpeakerVerification(model=model_name, num_speakers=100).to(device)
model.load_state_dict(torch.load("fine_tuned_wavlm.pth"))
model.to(device)
model.eval()

# Function to compute embeddings
def compute_embeddings(audio_file):
    """
    Computes embeddings for a given audio file.

    Args:
        audio_file (str): Path to the audio file.

    Returns:
        embedding (torch.Tensor): Embedding vector.
    """
    waveform, _ = torchaudio.load(audio_file)
    inputs = feature_extractor(waveform.squeeze().numpy(), return_tensors="pt", sampling_rate=16000)
    inputs = {key: value.to(device) for key, value in inputs.items()}

    with torch.no_grad():
        outputs = model(inputs["input_values"])
    embedding = outputs[0].mean(dim=1)  # Mean pooling
    return embedding

# Identify speakers
def identify_speakers(separated_sources, speaker_to_label):
    """
    Identifies speakers from separated sources.

    Args:
        separated_sources (list): List of separated waveforms.
        speaker_to_label (dict): Mapping from speaker IDs to labels.

    Returns:
        predictions (list): List of predicted speaker labels.
    """
    predictions = []
    for source in separated_sources:
        embedding = compute_embeddings(source)
        logits = model.classifier(embedding)
        predicted_label = torch.argmax(logits, dim=1).item()
        predictions.append(predicted_label)
    return predictions

# Evaluate Rank-1 accuracy
def compute_rank1_accuracy(predictions, true_labels):
    """
    Computes Rank-1 identification accuracy.

    Args:
        predictions (list): List of predicted labels.
        true_labels (list): List of true labels.

    Returns:
        accuracy (float): Rank-1 accuracy.
    """
    correct = sum(1 for pred, label in zip(predictions, true_labels) if pred == label)
    accuracy = correct / len(true_labels) * 100
    return accuracy

# Process test set
rank1_accuracies = []
for mixed_file in tqdm(test_mixed_files):
    separated_sources = separate_speakers(mixed_file)
    predictions = identify_speakers(separated_sources, speaker_to_label)
    rank1_accuracy = compute_rank1_accuracy(predictions, true_labels)
    rank1_accuracies.append(rank1_accuracy)

print(f"Mean Rank-1 Accuracy: {np.mean(rank1_accuracies):.2f}%")

from transformers import WavLMModel, Wav2Vec2FeatureExtractor
from speechbrain.pretrained import SepformerSeparation
import torch
import torchaudio

# Load pre-trained SepFormer model
sepformer_model = SepformerSeparation.from_hparams(
    source="speechbrain/sepformer-wsj02mix",
    savedir="pretrained_models/sepformer"
)

# Load pre-trained WavLM model
model_name = "microsoft/wavlm-base"
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)
wavlm_model = WavLMModel.from_pretrained(model_name).to(device)

# Add classifier for speaker identification
class WavLMForSpeakerVerification(torch.nn.Module):
    def __init__(self, model, num_speakers):
        super(WavLMForSpeakerVerification, self).__init__()
        self.wavlm = model
        self.classifier = torch.nn.Linear(self.wavlm.config.hidden_size, num_speakers)

    def forward(self, input_values):
        outputs = self.wavlm(input_values)
        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling
        logits = self.classifier(embeddings)
        return embeddings, logits

# Initialize speaker identification model
num_speakers = 100  # Adjust based on your dataset
speaker_model = WavLMForSpeakerVerification(wavlm_model, num_speakers).to(device)

class CombinedPipeline(torch.nn.Module):
    def __init__(self, sepformer_model, speaker_model):
        super(CombinedPipeline, self).__init__()
        self.sepformer = sepformer_model
        self.speaker_model = speaker_model

    def forward(self, mixed_audio):
        # Step 1: Separate speakers using SepFormer
        separated_sources = self.sepformer.separate_batch(mixed_audio.unsqueeze(0)).squeeze(0)

        # Step 2: Identify speakers using WavLM
        embeddings, logits = [], []
        for source in separated_sources:
            source_input = feature_extractor(source.numpy(), return_tensors="pt", sampling_rate=16000)
            source_input = {key: value.to(device) for key, value in source_input.items()}
            embedding, logit = self.speaker_model(source_input["input_values"])
            embeddings.append(embedding)
            logits.append(logit)

        return separated_sources, embeddings, logits

from torch.optim import Adam
from speechbrain.utils.metric_stats import MetricStats
from pesq import pesq

# Define loss functions
def separation_loss(original, separated):
    return torch.mean((original - separated) ** 2)

def identification_loss(logits, labels):
    return torch.nn.CrossEntropyLoss()(logits, labels)

# Optimizer
optimizer = Adam(combined_pipeline.parameters(), lr=1e-4)

# Training loop
def train_pipeline(train_files, combined_pipeline, num_epochs=10):
    for epoch in range(num_epochs):
        total_loss = 0
        for mixed_file, true_labels in tqdm(train_files):
            mixed_audio, _ = torchaudio.load(mixed_file)
            mixed_audio = mixed_audio.to(device)

            # Forward pass
            separated_sources, embeddings, logits = combined_pipeline(mixed_audio)

            # Compute losses
            sep_loss = separation_loss(mixed_audio, torch.stack(separated_sources))
            id_loss = identification_loss(torch.cat(logits), torch.tensor(true_labels).to(device))
            total_loss += sep_loss + id_loss

            # Backward pass
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss.item()}")

def evaluate_pipeline(test_files, combined_pipeline):
    sir_scores, sar_scores, sdr_scores, pesq_scores, rank1_accuracies = [], [], [], [], []

    for mixed_file, true_labels in tqdm(test_files):
        mixed_audio, _ = torchaudio.load(mixed_file)
        mixed_audio = mixed_audio.to(device)

        # Forward pass
        separated_sources, embeddings, logits = combined_pipeline(mixed_audio)

        # Evaluate metrics for each source
        for i, source in enumerate(separated_sources):
            original_signal = ...  # Load corresponding clean signal
            sdr = MetricStats(metric="si-sdr").append(ids=["test"], predictions=[source], targets=[original_signal])
            pesq_score = pesq(16000, original_signal.numpy(), source.numpy(), "wb")
            sir_scores.append(sdr["sir"])
            sar_scores.append(sdr["sar"])
            sdr_scores.append(sdr["sdr"])
            pesq_scores.append(pesq_score)

        # Compute Rank-1 accuracy
        predicted_labels = [torch.argmax(logit, dim=1).item() for logit in logits]
        rank1_accuracy = compute_rank1_accuracy(predicted_labels, true_labels)
        rank1_accuracies.append(rank1_accuracy)

    print(f"Mean SIR: {np.mean(sir_scores):.2f}")
    print(f"Mean SAR: {np.mean(sar_scores):.2f}")
    print(f"Mean SDR: {np.mean(sdr_scores):.2f}")
    print(f"Mean PESQ: {np.mean(pesq_scores):.2f}")
    print(f"Mean Rank-1 Accuracy: {np.mean(rank1_accuracies):.2f}%")

